{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SampleSparkCode\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Dinesh| 34|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [Row(\"Dinesh\",\"34\")]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Dinesh| 34|\n",
      "+------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 46988)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Champ\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from {df}\",df=df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+--------+-------------+-------------+----------+--------------------+--------------------+\n",
      "|AddressID|       AddressLine1|AddressLine2|    City|StateProvince|CountryRegion|PostalCode|             rowguid|        ModifiedDate|\n",
      "+---------+-------------------+------------+--------+-------------+-------------+----------+--------------------+--------------------+\n",
      "|        9|  8713 Yosemite Ct.|        NULL| Bothell|   Washington|United States|     98011|268af621-76d7-4c7...|2006-07-01 00:00:...|\n",
      "|       11|1318 Lasalle Street|        NULL| Bothell|   Washington|United States|     98011|981b3303-aca2-49c...|2007-04-01 00:00:...|\n",
      "|       25|   9178 Jumping St.|        NULL|  Dallas|        Texas|United States|     75201|c8df3bd9-48f0-465...|2006-09-01 00:00:...|\n",
      "|       28|   9228 Via Del Sol|        NULL| Phoenix|      Arizona|United States|     85004|12ae5ee1-fc3e-468...|2005-09-01 00:00:...|\n",
      "|       32|  26910 Indela Road|        NULL|Montreal|       Quebec|       Canada|   H1Y 2H5|84a95f62-3ae8-4e7...|2006-08-01 00:00:...|\n",
      "+---------+-------------------+------------+--------+-------------+-------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_address = spark.read.csv(\"..\\data\\Address.csv\",header=True)\n",
    "df_address.show(5)\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|count(1)|            City|\n",
      "+--------+----------------+\n",
      "|       1|      Pnot-Rouge|\n",
      "|       1|     Springfield|\n",
      "|       1| North Las Vegas|\n",
      "|       1|          Auburn|\n",
      "|       4|         Phoenix|\n",
      "|       2|        Winnipeg|\n",
      "|       1|   Lake Elsinore|\n",
      "|       1|       Bountiful|\n",
      "|       2|       Clackamas|\n",
      "|       1|          Monroe|\n",
      "|       1|        Westland|\n",
      "|       2|     Culver City|\n",
      "|       1|         Hanford|\n",
      "|       4|         Bothell|\n",
      "|       3|         Everett|\n",
      "|       6|          Ottawa|\n",
      "|       1|North Sioux City|\n",
      "|       1|        Lewiston|\n",
      "|       6|          Dallas|\n",
      "|       1|     Great Falls|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*), City from {df} group by City\",df=df_address).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_address = spark.read.csv(\"..\\data\\Address.csv\",header=True)\n",
    "df_customer = spark.read.csv(\"..\\data\\Customer.csv\",header=True)\n",
    "df_customer_address = spark.read.csv(\"..\\data\\CustomerAddress.csv\",header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_customer_address = df_customer.join(df_customer_address,df_customer.CustomerID==df_customer_address.CustomerID) \\\n",
    "                                    .join(df_address,df_customer_address.AddressID==df_address.AddressID)\n",
    "df_dim_customer_address.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|Address_Count|   StateProvince|\n",
      "+-------------+----------------+\n",
      "|            9|            Utah|\n",
      "|            1|        Manitoba|\n",
      "|            1|       Brunswick|\n",
      "|            7|       Minnesota|\n",
      "|           17|          Oregon|\n",
      "|           37|           Texas|\n",
      "|           13|         Alberta|\n",
      "|            7|          Nevada|\n",
      "|           48|      Washington|\n",
      "|           14|        Illinois|\n",
      "|            3|      New Mexico|\n",
      "|           11|        Missouri|\n",
      "|           51|         Ontario|\n",
      "|            3|         Montana|\n",
      "|           16|        Michigan|\n",
      "|           17|British Columbia|\n",
      "|            4|         Wyoming|\n",
      "|           24|          Quebec|\n",
      "|           13|         Arizona|\n",
      "|           38|         England|\n",
      "+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as Address_Count, StateProvince from {df} group by StateProvince\",df=df_dim_customer_address).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
