{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SampleSparkCode\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_address = spark.read.csv(\"..\\data\\Address.csv\",header=True)\n",
    "df_customer = spark.read.csv(\"..\\data\\Customer.csv\",header=True)\n",
    "df_customer_address = spark.read.csv(\"..\\data\\CustomerAddress.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address = df_customer.alias(\"c\").join(\n",
    "    df_customer_address.alias(\"ca\"), col(\"c.CustomerID\") ==  col(\"ca.CustomerID\")) \\\n",
    "        .join(df_address.alias(\"a\"),  col(\"ca.AddressID\") ==  col(\"a.AddressID\")) \\\n",
    "        .select(\n",
    "     col(\"c.CustomerID\").alias(\"CustomerID\"),\n",
    "     col(\"ca.AddressID\").alias(\"AddressID\"),\n",
    "    \"c.FirstName\", \"c.LastName\", \"c.CompanyName\", \"c.EmailAddress\", \"c.Phone\",\n",
    "    \"ca.AddressType\", \"a.AddressLine1\", \"a.AddressLine2\", \"a.City\", \"a.StateProvince\",\n",
    "    \"a.CountryRegion\", \"a.PostalCode\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "|CustomerID|AddressID|FirstName|   LastName|         CompanyName|        EmailAddress|       Phone|AddressType|     AddressLine1|AddressLine2|       City|StateProvince|CountryRegion|PostalCode|\n",
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "|     29485|     1086|Catherine|       Abel|Professional Sale...|catherine0@advent...|747-555-0171|Main Office|57251 Serene Blvd|        NULL|   Van Nuys|   California|United States|     91411|\n",
      "|     29486|      621|      Kim|Abercrombie|      Riders Company|kim2@adventure-wo...|334-555-0137|Main Office|   Tanger Factory|        NULL|     Branch|    Minnesota|United States|     55056|\n",
      "|     29489|     1069|  Frances|      Adams|Area Bike Accesso...|frances0@adventur...|991-555-0183|Main Office|   6900 Sisk Road|        NULL|    Modesto|   California|United States|     95354|\n",
      "|     29490|      887| Margaret|      Smith|Bicycle Accessori...|margaret0@adventu...|959-555-0151|Main Office|    Lewiston Mall|        NULL|   Lewiston|        Idaho|United States|     83501|\n",
      "|     29492|      618|      Jay|      Adams|Valley Bicycle Sp...|jay1@adventure-wo...|158-555-0142|Main Office|  Blue Ridge Mall|        NULL|Kansas City|     Missouri|United States|     64106|\n",
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_customer_address.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists dim_customer_address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partition by State\n",
    "- Parition by can have both save and saveAsTable. In Save, it can take any relative path and save the files there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .partitionBy(\"StateProvince\") \\\n",
    "    .option(\"path\",\"..\\processed\\example_partition_by\\dimcustomer\") \\\n",
    "    .saveAsTable(\"dim_customer_address\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bucket By State( Note that Bucket does not allow save(). It allows only save as table)\n",
    "- Remember that bucketBy does not take relative path. It points to the warehouse directory - typically where spark is running. This is where Spark's default metastore resides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists dim_customer_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .bucketBy(10, \"StateProvince\") \\\n",
    "    .sortBy(\"StateProvince\", \"City\") \\\n",
    "    .option(\"path\", \"..\\processed\\example_bucket_by\\dimcustomer\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"dim_customer_address\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
