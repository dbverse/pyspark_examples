{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Between `partitionBy` and `bucketBy` in Spark\n",
    "\n",
    "## `partitionBy`\n",
    "- **Operation Type**: Disk operation.\n",
    "- **Description**: \n",
    "  - Divides data into separate directories based on the values of specified columns.\n",
    "  - Each partition is written as a separate directory, which can enhance read performance by skipping irrelevant partitions during queries.\n",
    "  \n",
    "- **Advantages**:\n",
    "  - **Improved Query Performance**: Queries that filter on the partitioned column can read only the relevant partitions, reducing I/O.\n",
    "  - **Scalability**: Ideal for large datasets where certain columns are frequently queried.\n",
    "\n",
    "- **When to Use**:\n",
    "  - Use `partitionBy` when you frequently filter data based on specific columns (e.g., date, region).\n",
    "  - Suitable for data that is large and can benefit from partitioned storage structure.\n",
    "\n",
    "## `bucketBy`\n",
    "- **Operation Type**: Disk operation.\n",
    "- **Description**: \n",
    "  - Distributes data across a fixed number of buckets based on the hash of specified columns.\n",
    "  - Each bucket is a separate file in the output directory.\n",
    "\n",
    "- **Advantages**:\n",
    "  - **Improved Joins**: When joining tables that are bucketed on the same columns and have the same number of buckets, Spark can perform efficient joins.\n",
    "  - **Reduced Shuffle**: Helps in reducing shuffling when performing aggregations or joins on bucketed columns.\n",
    "\n",
    "- **When to Use**:\n",
    "  - Use `bucketBy` when you need to optimize joins or aggregations between large datasets.\n",
    "  - Suitable for scenarios where data distribution across buckets can help enhance performance.\n",
    "\n",
    "## Summary\n",
    "- Use **`partitionBy`** for optimizing query performance based on filter conditions.\n",
    "- Use **`bucketBy`** for optimizing join and aggregation operations between large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SampleSparkCode\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_address = spark.read.csv(\"..\\data\\Address.csv\",header=True)\n",
    "df_customer = spark.read.csv(\"..\\data\\Customer.csv\",header=True)\n",
    "df_customer_address = spark.read.csv(\"..\\data\\CustomerAddress.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address = df_customer.alias(\"c\").join(\n",
    "    df_customer_address.alias(\"ca\"), col(\"c.CustomerID\") ==  col(\"ca.CustomerID\")) \\\n",
    "        .join(df_address.alias(\"a\"),  col(\"ca.AddressID\") ==  col(\"a.AddressID\")) \\\n",
    "        .select(\n",
    "     col(\"c.CustomerID\").alias(\"CustomerID\"),\n",
    "     col(\"ca.AddressID\").alias(\"AddressID\"),\n",
    "    \"c.FirstName\", \"c.LastName\", \"c.CompanyName\", \"c.EmailAddress\", \"c.Phone\",\n",
    "    \"ca.AddressType\", \"a.AddressLine1\", \"a.AddressLine2\", \"a.City\", \"a.StateProvince\",\n",
    "    \"a.CountryRegion\", \"a.PostalCode\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "|CustomerID|AddressID|FirstName|   LastName|         CompanyName|        EmailAddress|       Phone|AddressType|     AddressLine1|AddressLine2|       City|StateProvince|CountryRegion|PostalCode|\n",
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "|     29485|     1086|Catherine|       Abel|Professional Sale...|catherine0@advent...|747-555-0171|Main Office|57251 Serene Blvd|        NULL|   Van Nuys|   California|United States|     91411|\n",
      "|     29486|      621|      Kim|Abercrombie|      Riders Company|kim2@adventure-wo...|334-555-0137|Main Office|   Tanger Factory|        NULL|     Branch|    Minnesota|United States|     55056|\n",
      "|     29489|     1069|  Frances|      Adams|Area Bike Accesso...|frances0@adventur...|991-555-0183|Main Office|   6900 Sisk Road|        NULL|    Modesto|   California|United States|     95354|\n",
      "|     29490|      887| Margaret|      Smith|Bicycle Accessori...|margaret0@adventu...|959-555-0151|Main Office|    Lewiston Mall|        NULL|   Lewiston|        Idaho|United States|     83501|\n",
      "|     29492|      618|      Jay|      Adams|Valley Bicycle Sp...|jay1@adventure-wo...|158-555-0142|Main Office|  Blue Ridge Mall|        NULL|Kansas City|     Missouri|United States|     64106|\n",
      "+----------+---------+---------+-----------+--------------------+--------------------+------------+-----------+-----------------+------------+-----------+-------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_customer_address.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists dim_customer_address\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partition by State\n",
    "- Parition by can have both save and saveAsTable. In Save, it can take any relative path and save the files there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .partitionBy(\"StateProvince\") \\\n",
    "    .option(\"path\",\"..\\processed\\example_partition_by\\dimcustomer\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"dim_customer_address\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bucket By State( Note that Bucket does not allow save(). It allows only save as table)\n",
    "- Based on the number given in bucketby, the files will be created (here 10 parquet files)\n",
    "- Remember that bucketBy does not take relative path. It points to the warehouse directory - typically where spark is running. This is where Spark's default metastore resides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists dim_customer_address\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .bucketBy(10, \"StateProvince\") \\\n",
    "    .option(\"path\", \"..\\processed\\example_bucket_by\\dimcustomer\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"dim_customer_address\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below is syntanctially correct, but it will not write to disk . Instead its in-memory only. Spark does not consider this partition by and still uses its own logical plan whenver this dataframe is filtered or queries or applied any transformation. So parition by or bucket by are useless unless its written to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x25ce2a99590>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .partitionBy(\"StateProvince\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Writing this table without partition or bucket by saves the table as only one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_partitions = df_dim_customer_address.rdd.getNumPartitions()\n",
    "print(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists dim_customer_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer_address.write.format(\"parquet\") \\\n",
    "    .option(\"path\", \"..\\processed\\plain_write\\dimcustomer\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"dim_customer_address\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
